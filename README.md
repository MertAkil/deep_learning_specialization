# Deep Learning Specilization (Coursera)

This repository documents my learning journey through the <b> Deep Learning Specilization</b> offered by `deeplearning.ai`.
<br>
In this repository, I will showcase the projects and assignments completed as part of the specilization.

## Course 1: Neural Networks and Deep Learning
In this course, the fundamentals of Neural Networks are covered. Starting with Logistic Regression, we explore how it relates to Neural Networks, progressing to building custom Neural Networks from scratch using NumPy. The final project involves creating a custom Deep Neural Network for image classification using the foundational concepts learned throughout the course.

## Course 2: Improvin Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
Training neural networks can present various challenges. This course covers several techniques to address these issues, explaining the mathematical foundations behind each. Topics include weight initialization strategies (such as zeros, random, and He initialization), model regularization techniques (L2 regularization, dropout, early stopping, data augmentation), and gradient checking.
<br>
Additionally, various optimization methods beyond standard gradient descent are introduced, such as the Adam optimizer.
<br>
In the final week of the course, TensorFlow is introduced, and it is used to train a neural network.

## Course 3: Structuring Machine Learning Projects
Although this course does not include assignments or projects, it offers valuable lessons for structuring machine learning projects effectively. One key takeaway is the importance of orthogonalization in machine learning systems and experiments. Orthogonalization ensures that adjusting one part of the system does not unintentionally affect other parts, allowing for clearer experimentation and improvement.
<br>
The course also emphasizes the value of having a single evaluation metric and discusses common pitfalls when choosing evaluation criteria. Additionally, the concept of Human-Level Performance is explored in the context of Bayes optimal error, and the challenges of surpassing human-level performance are examined through various case studies. The course also covers avoidable bias and expectations in model performance.
<br>
Later sections introduce key principles such as error analysis, data cleaning, prototyping and iterating quickly, and dealing with train/test set distribution mismatches and data mismatches.
<br>
Finally, the course provides an introduction to Transfer Learning and Multi-Task Learning, using real-world examples to demonstrate the applications of these techniques. It also covers End-to-End Deep Learning, outlining both its advantages and limitations.

## Course 4: Convolutional Neural Networks
This course focuses entirely on Convolutional Neural Networks (CNNs), covering both the motivation behind CNNs and the mathematical concepts involved. In the first project, a CNN is implemented from scratch. In the second project, I implemented a CNN using TensorFlow's Sequential API and Functional API for both binary and multi-task classification problems.
<br>
Additionally, several advanced models are introduced and implemented, such as Residual Networks (ResNet) and MobileNet. Transfer Learning is also applied to fine-tune MobileNet for a specific task.
<br>
In the latter part of the course, specialized applications of CNNs are explored and implemented, including Object Detection, Image Segmentation, Neural Style Transfer, and Face Recognition.

## Course 5: Sequence Models
In this final course of the specialization, Recurrent Neural Networks (RNNs) are introduced, with a focus on their motivation and applications. The course covers key RNN architectures, including GRU and LSTM, which are used in projects such as character-level language modeling and jazz music generation.
<br>
In the second week, the course shifts to Word Vector Representations, introducing and using Word2Vec and GloVe for natural language processing tasks.
<br>
The latter half of the course delves into Attention mechanisms and Transformer Networks, applying them to various tasks like Neural Machine Translation, Trigger Word Detection, Named-Entity Recognition, and Question Answering.

